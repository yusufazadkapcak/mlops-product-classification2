# Dockerfile for inference API
FROM python:3.10-slim

# Set the working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements file
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt && \
    pip install --no-cache-dir boto3

# Copy source code
COPY src/ ./src/
COPY models/ ./models/

# Create necessary directories if they don't exist
RUN mkdir -p models artifacts

# Set Python path
ENV PYTHONPATH=/app

# Environment variables for cloud deployment
ENV PORT=8000
ENV HOST=0.0.0.0
ENV MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-file:./mlruns}

# Expose the port for the API
EXPOSE 8000

# Command to run the API (uses PORT from environment for Railway)
CMD python -m uvicorn src.inference.api:app --host ${HOST} --port ${PORT}
